{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":128753,"databundleVersionId":15438372}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/ml-challenge-udhgam-2/train.jsonl\"\nTEST_PATH  = \"/kaggle/input/ml-challenge-udhgam-2/test.jsonl\"\n\ndef load_jsonl_safe(path):\n    data=[]\n    with open(path) as f:\n        for line in f:\n            try:\n                data.append(json.loads(line))\n            except:\n                pass\n    return pd.DataFrame(data)\n\ntrain_df = load_jsonl_safe(TRAIN_PATH)\ntest_df  = load_jsonl_safe(TEST_PATH)\n\nprint(train_df.shape, test_df.shape)\nprint(train_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T17:11:36.449501Z","iopub.execute_input":"2026-02-02T17:11:36.449731Z","iopub.status.idle":"2026-02-02T17:11:39.700426Z","shell.execute_reply.started":"2026-02-02T17:11:36.449695Z","shell.execute_reply":"2026-02-02T17:11:39.699739Z"}},"outputs":[{"name":"stdout","text":"(79806, 4) (19952, 3)\nIndex(['example_id', 'input_ids', 'attention_mask', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport json\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom gensim.models import Word2Vec\n\n# --- Config ---\nCONFIG = {\n    'seed': 42,\n    'epochs': 6,            # Optimized for pretrained convergence\n    'batch_size': 32,\n    'lr': 5e-4,\n    'max_len': 512,\n    'embed_dim': 128,       # Must match Word2Vec vector_size\n    'n_layers': 3,\n    'n_heads': 4,\n    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(CONFIG['seed'])\nprint(f\"Using device: {CONFIG['device']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:18:27.318934Z","iopub.execute_input":"2026-02-02T19:18:27.319548Z","iopub.status.idle":"2026-02-02T19:18:49.860965Z","shell.execute_reply.started":"2026-02-02T19:18:27.319522Z","shell.execute_reply":"2026-02-02T19:18:49.860310Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n  from google.cloud.aiplatform.utils import gcs_utils\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# -----------------------------\n# 2. Data Loading\n# -----------------------------\nprint(\"\\n--- Loading Data ---\")\ntrain_path = \"/kaggle/input/ml-challenge-udhgam-2/train.jsonl\"\ntest_path  = \"/kaggle/input/ml-challenge-udhgam-2/test.jsonl\"\n\ndef load_jsonl(path):\n    data = []\n    with open(path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return pd.DataFrame(data)\n\ntrain_df = load_jsonl(train_path)\ntest_df = load_jsonl(test_path)\n\n# Dynamic Vocab Calculation\nmax_id_train = max([max(x) for x in train_df['input_ids'] if len(x) > 0], default=0)\nmax_id_test = max([max(x) for x in test_df['input_ids'] if len(x) > 0], default=0)\nVOCAB_SIZE = max(max_id_train, max_id_test) + 100\nprint(f\"Vocabulary Size: {VOCAB_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:19:07.646952Z","iopub.execute_input":"2026-02-02T19:19:07.647582Z","iopub.status.idle":"2026-02-02T19:19:10.966554Z","shell.execute_reply.started":"2026-02-02T19:19:07.647554Z","shell.execute_reply":"2026-02-02T19:19:10.965736Z"}},"outputs":[{"name":"stdout","text":"\n--- Loading Data ---\nVocabulary Size: 50467\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# -----------------------------\n# 3. Word2Vec Pretraining\n# -----------------------------\nprint(\"\\n--- Training Word2Vec Embeddings ---\")\n# Combine all data to learn token relationships\nall_sentences = train_df['input_ids'].tolist() + test_df['input_ids'].tolist()\n# Convert to strings for Gensim (Gensim requires string tokens)\nall_sentences_str = [[str(x) for x in seq] for seq in all_sentences]\n\n# Train W2V\nw2v = Word2Vec(sentences=all_sentences_str, vector_size=CONFIG['embed_dim'], \n               window=5, min_count=1, workers=4, epochs=10, seed=CONFIG['seed'])\n\n# Extract Weights into a matrix\nembedding_matrix = np.zeros((VOCAB_SIZE, CONFIG['embed_dim']))\nfor i in range(VOCAB_SIZE):\n    token = str(i)\n    if token in w2v.wv:\n        embedding_matrix[i] = w2v.wv[token]\n    else:\n        # Random init for unseen tokens\n        embedding_matrix[i] = np.random.normal(scale=0.1, size=(CONFIG['embed_dim'],))\n\npretrained_embeddings = torch.tensor(embedding_matrix, dtype=torch.float32)\nprint(\"Word2Vec Ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:19:19.222010Z","iopub.execute_input":"2026-02-02T19:19:19.222354Z","iopub.status.idle":"2026-02-02T19:20:19.958279Z","shell.execute_reply.started":"2026-02-02T19:19:19.222315Z","shell.execute_reply":"2026-02-02T19:20:19.957607Z"}},"outputs":[{"name":"stdout","text":"\n--- Training Word2Vec Embeddings ---\nWord2Vec Ready!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# -----------------------------\n# 4. Feature Engineering (For LightGBM)\n# -----------------------------\nprint(\"\\n--- Engineering Features for LightGBM ---\")\ndef get_handcrafted_features(input_ids):\n    length = len(input_ids)\n    if length == 0: return [0]*6\n    arr = np.array(input_ids)\n    return [length, len(np.unique(arr)), 1 - (len(np.unique(arr))/length), \n            np.mean(arr), np.std(arr), np.max(arr)]\n\ntrain_feats = np.array([get_handcrafted_features(x) for x in train_df['input_ids']])\ntest_feats = np.array([get_handcrafted_features(x) for x in test_df['input_ids']])\n\n# TF-IDF + SVD\ntrain_str = train_df['input_ids'].apply(lambda x: ' '.join(map(str, x)))\ntest_str = test_df['input_ids'].apply(lambda x: ' '.join(map(str, x)))\n\ntfidf = TfidfVectorizer(max_features=15000, token_pattern=r'\\b\\w+\\b')\ntrain_tfidf = tfidf.fit_transform(train_str)\ntest_tfidf = tfidf.transform(test_str)\n\nsvd = TruncatedSVD(n_components=32, random_state=CONFIG['seed'])\ntrain_svd = svd.fit_transform(train_tfidf)\ntest_svd = svd.transform(test_tfidf)\n\nX_train_gbm = np.hstack([train_feats, train_svd])\nX_test_gbm = np.hstack([test_feats, test_svd])\n\nscaler = StandardScaler()\nX_train_gbm = scaler.fit_transform(X_train_gbm)\nX_test_gbm = scaler.transform(X_test_gbm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:20:45.880342Z","iopub.execute_input":"2026-02-02T19:20:45.880836Z","iopub.status.idle":"2026-02-02T19:21:00.942407Z","shell.execute_reply.started":"2026-02-02T19:20:45.880806Z","shell.execute_reply":"2026-02-02T19:21:00.941801Z"}},"outputs":[{"name":"stdout","text":"\n--- Engineering Features for LightGBM ---\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# -----------------------------\n# 5. PyTorch Dataset & Collator\n# -----------------------------\nclass ObfuscatedDataset(Dataset):\n    def __init__(self, df, labels=None):\n        self.input_ids = df['input_ids'].tolist()\n        self.labels = labels\n    def __len__(self): return len(self.input_ids)\n    def __getitem__(self, idx):\n        ids = torch.tensor(self.input_ids[idx], dtype=torch.long)\n        if self.labels is not None:\n            return ids, torch.tensor(self.labels[idx], dtype=torch.float)\n        return ids\n\ndef collate_fn(batch):\n    if isinstance(batch[0], tuple):\n        ids, labels = zip(*batch)\n        ids_pad = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids_pad, (ids_pad != 0).float(), torch.stack(labels)\n    else:\n        ids = batch\n        ids_pad = pad_sequence(ids, batch_first=True, padding_value=0)\n        return ids_pad, (ids_pad != 0).float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:21:27.038280Z","iopub.execute_input":"2026-02-02T19:21:27.039024Z","iopub.status.idle":"2026-02-02T19:21:27.044760Z","shell.execute_reply.started":"2026-02-02T19:21:27.038992Z","shell.execute_reply":"2026-02-02T19:21:27.044084Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# -----------------------------\n# 6. Model Architectures\n# -----------------------------\nclass AttentionPooling(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(in_dim, in_dim), nn.LayerNorm(in_dim), nn.Tanh(), nn.Linear(in_dim, 1)\n        )\n    def forward(self, last_hidden_state, mask):\n        w = self.attention(last_hidden_state).squeeze(-1)\n        w = w.masked_fill(mask == 0, -1e4)\n        w = torch.softmax(w, dim=1)\n        return torch.sum(last_hidden_state * w.unsqueeze(-1), dim=1)\n\n# Model 1: Transformer\nclass TransformerModel(nn.Module):\n    def __init__(self, pretrained_emb):\n        super().__init__()\n        # Load Word2Vec weights\n        self.embedding = nn.Embedding.from_pretrained(pretrained_emb, freeze=False, padding_idx=0)\n        self.pos_emb = nn.Embedding(CONFIG['max_len'], CONFIG['embed_dim'])\n        enc_layer = nn.TransformerEncoderLayer(d_model=CONFIG['embed_dim'], nhead=CONFIG['n_heads'], \n                                             dim_feedforward=512, dropout=0.1, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=CONFIG['n_layers'])\n        self.pooler = AttentionPooling(CONFIG['embed_dim'])\n        self.head = nn.Sequential(nn.Linear(CONFIG['embed_dim'], 64), nn.ReLU(), \n                                  nn.Dropout(0.1), nn.Linear(64, 1), nn.Sigmoid())\n\n    def forward(self, ids, mask):\n        if ids.size(1) > CONFIG['max_len']: ids, mask = ids[:, :CONFIG['max_len']], mask[:, :CONFIG['max_len']]\n        x = self.embedding(ids) + self.pos_emb(torch.arange(ids.size(1), device=ids.device).unsqueeze(0))\n        x = self.transformer(x, src_key_padding_mask=(mask==0))\n        return self.head(self.pooler(x, mask)).squeeze()\n\n# Model 2: Bi-Directional GRU\nclass GRUModel(nn.Module):\n    def __init__(self, pretrained_emb):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(pretrained_emb, freeze=False, padding_idx=0)\n        self.gru = nn.GRU(CONFIG['embed_dim'], 64, num_layers=2, batch_first=True, bidirectional=True)\n        self.pooler = AttentionPooling(128) # 64 * 2 (Bidirectional)\n        self.head = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), \n                                  nn.Dropout(0.1), nn.Linear(64, 1), nn.Sigmoid())\n\n    def forward(self, ids, mask):\n        if ids.size(1) > CONFIG['max_len']: ids, mask = ids[:, :CONFIG['max_len']], mask[:, :CONFIG['max_len']]\n        x = self.embedding(ids)\n        x, _ = self.gru(x)\n        return self.head(self.pooler(x, mask)).squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:22:00.960588Z","iopub.execute_input":"2026-02-02T19:22:00.960885Z","iopub.status.idle":"2026-02-02T19:22:00.972336Z","shell.execute_reply.started":"2026-02-02T19:22:00.960842Z","shell.execute_reply":"2026-02-02T19:22:00.971704Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# -----------------------------\n# 7. Training Engine\n# -----------------------------\ndef train_model(model_class, model_name, train_df, labels, pretrained_emb):\n    kf = KFold(n_splits=5, shuffle=True, random_state=CONFIG['seed'])\n    oof_preds, test_preds = np.zeros(len(train_df)), np.zeros(len(test_df))\n    \n    test_loader = DataLoader(ObfuscatedDataset(test_df), batch_size=CONFIG['batch_size']*2, \n                             collate_fn=collate_fn, shuffle=False)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        print(f\"--- {model_name} Fold {fold+1} ---\")\n        train_loader = DataLoader(ObfuscatedDataset(train_df.iloc[train_idx], labels[train_idx]), \n                                  batch_size=CONFIG['batch_size'], collate_fn=collate_fn, shuffle=True)\n        val_loader = DataLoader(ObfuscatedDataset(train_df.iloc[val_idx], labels[val_idx]), \n                                batch_size=CONFIG['batch_size']*2, collate_fn=collate_fn, shuffle=False)\n        \n        model = model_class(pretrained_emb).to(CONFIG['device'])\n        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CONFIG['lr'], \n                                                        steps_per_epoch=len(train_loader), epochs=CONFIG['epochs'])\n        criterion = nn.L1Loss()\n        scaler = torch.amp.GradScaler('cuda')\n\n        for epoch in range(CONFIG['epochs']):\n            model.train()\n            for ids, mask, y in train_loader:\n                ids, mask, y = ids.to(CONFIG['device']), mask.to(CONFIG['device']), y.to(CONFIG['device'])\n                optimizer.zero_grad()\n                with torch.amp.autocast('cuda'):\n                    loss = criterion(model(ids, mask), y)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n\n        model.eval()\n        val_preds_fold = []\n        with torch.no_grad():\n            for ids, mask, y in val_loader:\n                val_preds_fold.extend(model(ids.to(CONFIG['device']), mask.to(CONFIG['device'])).cpu().numpy())\n        oof_preds[val_idx] = np.array(val_preds_fold)\n        \n        test_preds_fold = []\n        with torch.no_grad():\n            for ids, mask in test_loader:\n                test_preds_fold.extend(model(ids.to(CONFIG['device']), mask.to(CONFIG['device'])).cpu().numpy())\n        test_preds += np.array(test_preds_fold) / 5\n    \n    print(f\"{model_name} CV MAE: {mean_absolute_error(labels, oof_preds):.5f}\")\n    return oof_preds, test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:22:39.432181Z","iopub.execute_input":"2026-02-02T19:22:39.432767Z","iopub.status.idle":"2026-02-02T19:22:39.442988Z","shell.execute_reply.started":"2026-02-02T19:22:39.432742Z","shell.execute_reply":"2026-02-02T19:22:39.442264Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# -----------------------------\n# 8. Execution & Submission\n# -----------------------------\nlabels = train_df['label'].values\n\n# 1. Train Transformer\nprint(\"\\n=== Training Transformer ===\")\noof_trans, pred_trans = train_model(TransformerModel, \"Transformer\", train_df, labels, pretrained_embeddings)\n\n# 2. Train GRU (Added Diversity)\nprint(\"\\n=== Training Bi-GRU ===\")\noof_gru, pred_gru = train_model(GRUModel, \"Bi-GRU\", train_df, labels, pretrained_embeddings)\n\n# 3. Train LightGBM\nprint(\"\\n=== Training LightGBM ===\")\nkf = KFold(n_splits=5, shuffle=True, random_state=CONFIG['seed'])\noof_lgb, pred_lgb = np.zeros(len(labels)), np.zeros(len(test_df))\nparams = {'objective': 'mae', 'metric': 'mae', 'verbosity': -1, 'learning_rate': 0.05, \n          'num_leaves': 32, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_gbm)):\n    model = lgb.train(params, lgb.Dataset(X_train_gbm[tr_idx], labels[tr_idx]), num_boost_round=3000,\n                      valid_sets=[lgb.Dataset(X_train_gbm[val_idx], labels[val_idx])],\n                      callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])\n    oof_lgb[val_idx] = model.predict(X_train_gbm[val_idx])\n    pred_lgb += model.predict(X_test_gbm) / 5\nprint(f\"LightGBM CV MAE: {mean_absolute_error(labels, oof_lgb):.5f}\")\n\n# 4. Final Ensemble (Inverse Error Weighting)\nprint(\"\\n=== Blending Models ===\")\nscores = np.array([mean_absolute_error(labels, oof_trans),\n                   mean_absolute_error(labels, oof_gru),\n                   mean_absolute_error(labels, oof_lgb)])\n\n# Calculate weights: Better score (lower error) -> Higher weight\nweights = 1 / scores\nweights /= weights.sum()\n\nprint(f\"Final Weights -> Trans: {weights[0]:.3f}, GRU: {weights[1]:.3f}, LGB: {weights[2]:.3f}\")\n\nfinal_preds = (pred_trans * weights[0]) + (pred_gru * weights[1]) + (pred_lgb * weights[2])\nfinal_preds = np.clip(final_preds, 0, 1)\n\npd.DataFrame({\"example_id\": test_df.example_id, \"label\": final_preds}).to_csv(\"submission.csv\", index=False)\nprint(\"Submission Generated Successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T19:22:56.187077Z","iopub.execute_input":"2026-02-02T19:22:56.187793Z","iopub.status.idle":"2026-02-02T19:51:39.953139Z","shell.execute_reply.started":"2026-02-02T19:22:56.187766Z","shell.execute_reply":"2026-02-02T19:51:39.952355Z"}},"outputs":[{"name":"stdout","text":"\n=== Training Transformer ===\n--- Transformer Fold 1 ---\n--- Transformer Fold 2 ---\n--- Transformer Fold 3 ---\n--- Transformer Fold 4 ---\n--- Transformer Fold 5 ---\nTransformer CV MAE: 0.16026\n\n=== Training Bi-GRU ===\n--- Bi-GRU Fold 1 ---\n--- Bi-GRU Fold 2 ---\n--- Bi-GRU Fold 3 ---\n--- Bi-GRU Fold 4 ---\n--- Bi-GRU Fold 5 ---\nBi-GRU CV MAE: 0.15418\n\n=== Training LightGBM ===\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2907]\tvalid_0's l1: 0.180393\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2391]\tvalid_0's l1: 0.182203\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[3000]\tvalid_0's l1: 0.178003\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2998]\tvalid_0's l1: 0.179903\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2998]\tvalid_0's l1: 0.178743\nLightGBM CV MAE: 0.17985\n\n=== Blending Models ===\nFinal Weights -> Trans: 0.341, GRU: 0.355, LGB: 0.304\nSubmission Generated Successfully!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}